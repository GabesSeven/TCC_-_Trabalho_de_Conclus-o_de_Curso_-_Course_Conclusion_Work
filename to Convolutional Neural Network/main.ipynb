{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQtg9gblH-EN"
      },
      "source": [
        "!pip install -U numpy keras pandas tensorflow matplotlib sklearn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6WTKpN2IFAg"
      },
      "source": [
        "# Convolutional Neural Network (CNN) algorithm based on Marsh work (https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing).\n",
        "#\n",
        "# Algorithm Developed to carry out the Course Conclusion Work (Trabalho de Conclusão de Curso - TCC) in Computer Science \n",
        "# at Universidade Estadual Paulista Júlio de Mesquita Filho Campus of Bauru (UNESP)\n",
        "# Work developed by Gabriel Vieira under the guidance of Prof. Dr. Kelton Costa  \n",
        "#\n",
        "# Algorithm responsible for the execution of the neural network, \n",
        "# that is, the entire file configuration, model configuration, training and testing.\n",
        "#\n",
        "# !pip install -U numpy keras pandas tensorflow matplotlib sklearn \n",
        "\n",
        "import itertools\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#--- Function for create Confusion Matrix\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "        \n",
        "#--- Getting access to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)\n",
        "\n",
        "#--- Moving to current folder\n",
        "path = '/content/drive/My Drive/TCC_code_database/'\n",
        "os.chdir(path)\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "IMAGE_WIDTH = 640\n",
        "IMAGE_HEIGHT = 480\n",
        "IMAGE_CHANNELS = 3\n",
        "SAMPLE_SIZE = 3500 # the number of images we use from each of the three classes\n",
        "\n",
        "# #--- List amoung of 'training' folder content \n",
        "# print(len(os.listdir('input/train')))\n",
        "# print(len(os.listdir('input/test')))\n",
        "\n",
        "#--- Create a Dataframe containing all images\n",
        "df_data = pd.read_csv('input/training.csv')\n",
        "print(df_data.shape)\n",
        "\n",
        "#--- Check the class distribution\n",
        "df_data['class'].value_counts()\n",
        "\n",
        "#--- Create the Train and Val Sets\n",
        "print(df_data.head())\n",
        "\n",
        "#--- Balance the target distribution\n",
        "# We will reduce the number of samples in class 0.\n",
        "# take a random sample of class 0 with size equal to num samples in class 1 and class 2\n",
        "df_seam_carving = df_data[df_data['class'] == 'seam_carving'].sample(SAMPLE_SIZE, random_state = 101)\n",
        "df_seam_insertion = df_data[df_data['class'] == 'seam_insertion'].sample(SAMPLE_SIZE, random_state = 101)\n",
        "df_uncompressed = df_data[df_data['class'] == 'uncompressed'].sample(SAMPLE_SIZE, random_state = 101)\n",
        "\n",
        "df_data = pd.concat([df_seam_carving, df_seam_insertion, df_uncompressed], axis=0).reset_index(drop=True)\n",
        "df_data = shuffle(df_data) \n",
        "print(df_data['class'].value_counts())\n",
        "print(df_data.head())\n",
        "\n",
        "#--- Stratify=y creates a balanced validation set\n",
        "y = df_data['class']\n",
        "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
        "print(df_train)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "print(df_train['class'].value_counts())\n",
        "print(df_val['class'].value_counts())\n",
        "\n",
        "#--- Create a Directory Structure\n",
        "base_dir = 'base_dir'\n",
        "# os.mkdir(base_dir)\n",
        "\n",
        "# #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
        "# # now we create 2 folders inside 'base_dir':\n",
        "# # train_dir\n",
        "#     # a_seam_carving\n",
        "#     # b_seam_insertion\n",
        "#     # c_uncompressed\n",
        "\n",
        "# # val_dir\n",
        "#     # a_seam_carving\n",
        "#     # b_seam_insertion\n",
        "#     # c_uncompressed\n",
        "\n",
        "# train_dir\n",
        "train_dir = os.path.join(base_dir, 'train_dir')\n",
        "# os.mkdir(train_dir)\n",
        "\n",
        "# val_dir\n",
        "val_dir = os.path.join(base_dir, 'val_dir')\n",
        "# os.mkdir(val_dir)\n",
        "\n",
        "# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
        "# Inside each folder we create seperate folders for each class\n",
        "\n",
        "# # create new folders inside train_dir\n",
        "# seam_carving = os.path.join(train_dir, 'seam_carving')\n",
        "# os.mkdir(seam_carving)\n",
        "# seam_insertion = os.path.join(train_dir, 'seam_insertion')\n",
        "# os.mkdir(seam_insertion)\n",
        "# uncompressed = os.path.join(train_dir, 'uncompressed')\n",
        "# os.mkdir(uncompressed)\n",
        "\n",
        "# # create new folders inside val_dir\n",
        "# seam_carving = os.path.join(val_dir, 'seam_carving')\n",
        "# os.mkdir(seam_carving)\n",
        "# seam_insertion = os.path.join(val_dir, 'seam_insertion')\n",
        "# os.mkdir(seam_insertion)\n",
        "# uncompressed = os.path.join(val_dir, 'uncompressed')\n",
        "# os.mkdir(uncompressed)\n",
        "\n",
        "# check that the folders have been created\n",
        "print(os.listdir('base_dir/train_dir'))\n",
        "\n",
        "# Transfer the images into the folders\n",
        "# Set the id as the index in df_data\n",
        "df_data.set_index('filename', inplace=True)\n",
        "# Get a list of train and val images\n",
        "train_list = list(df_train['filename'])\n",
        "val_list = list(df_val['filename'])\n",
        "\n",
        "# # Transfer the train images\n",
        "# for image in train_list:\n",
        "#     # get the label for a certain image\n",
        "#     target = df_data.loc[image,'class']\n",
        "    \n",
        "#     # these must match the folder names\n",
        "#     if target == 'seam_carving':\n",
        "#         label = 'seam_carving'\n",
        "#     if target == 'seam_insertion':\n",
        "#         label = 'seam_insertion'\n",
        "#     if target == 'uncompressed':\n",
        "#         label = 'uncompressed'\n",
        "\n",
        "#     # source path to image\n",
        "#     src = os.path.join('input/train', image)\n",
        "#     # destination path to image\n",
        "#     dst = os.path.join(train_dir, label, image)\n",
        "#     # copy the image from the source to the destination\n",
        "#     try:\n",
        "#       shutil.copyfile(src, dst)\n",
        "#     except:\n",
        "#       continue \n",
        "\n",
        "# # Transfer the val images\n",
        "# for image in val_list:\n",
        "#     # get the label for a certain image\n",
        "#     target = df_data.loc[image,'class']\n",
        "\n",
        "#     # these must match the folder names\n",
        "#     if target == 'seam_carving':\n",
        "#         label = 'seam_carving'\n",
        "#     if target == 'seam_insertion':\n",
        "#         label = 'seam_insertion'\n",
        "#     if target == 'uncompressed':\n",
        "#         label = 'uncompressed'\n",
        "    \n",
        "#     # source path to image\n",
        "#     src = os.path.join('input/train', image)\n",
        "#     # destination path to image\n",
        "#     dst = os.path.join(val_dir, label, image)\n",
        "#     # copy the image from the source to the destination\n",
        "#     try:\n",
        "#       shutil.copyfile(src, dst)\n",
        "#     except:\n",
        "#       continue \n",
        "\n",
        "# # check how many train images we have in each folder\n",
        "print(len(os.listdir('base_dir/train_dir/seam_carving')))\n",
        "print(len(os.listdir('base_dir/train_dir/seam_insertion')))\n",
        "print(len(os.listdir('base_dir/train_dir/uncompressed')))\n",
        "\n",
        "# check how many val images we have in each folder\n",
        "print(len(os.listdir('base_dir/val_dir/seam_carving')))\n",
        "print(len(os.listdir('base_dir/val_dir/seam_insertion')))\n",
        "print(len(os.listdir('base_dir/val_dir/uncompressed')))\n",
        "\n",
        "#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n",
        "# We will be feeding test images from a folder into predict_generator().\n",
        "# Keras requires that the path should point to a folder containing images and not\n",
        "# to the images themselves. That is why we are creating a folder (test_images) \n",
        "# inside another folder (test_dir).\n",
        "\n",
        "# test_dir\n",
        "    # test_images\n",
        "\n",
        "# create test_dir\n",
        "test_dir = 'test_dir'\n",
        "# os.mkdir(test_dir)\n",
        "    \n",
        "# create test_images inside test_dir\n",
        "test_images = os.path.join(test_dir, 'test_images')\n",
        "# os.mkdir(test_images)\n",
        "# check that the directory we created exists\n",
        "print(len(os.listdir('test_dir')))\n",
        "\n",
        "# Transfer the test images into image_dir\n",
        "test_list = os.listdir('input/test')\n",
        "\n",
        "# for image in test_list:\n",
        "#     # source path to image\n",
        "#     src = os.path.join('input/test', image)\n",
        "#     # destination path to image\n",
        "#     dst = os.path.join(test_images, image)\n",
        "#     # copy the image from the source to the destination\n",
        "#     shutil.copyfile(src, dst)\n",
        "# # check that the images are now in the test_images\n",
        "# # Should now be 57458 images in the test_images folder\n",
        "# print(len(os.listdir('test_dir/test_images')))\n",
        "\n",
        "#--- Set Up the Generators Train\n",
        "train_path = 'base_dir/train_dir'\n",
        "valid_path = 'base_dir/val_dir'\n",
        "test_path = 'input/test'\n",
        "\n",
        "num_train_samples = len(df_train)\n",
        "num_val_samples = len(df_val)\n",
        "train_batch_size = 10\n",
        "val_batch_size = 10\n",
        "\n",
        "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
        "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
        "datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(train_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=train_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "val_gen = datagen.flow_from_directory(valid_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=val_batch_size,\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "# Note: shuffle=False causes the test dataset to not be shuffled\n",
        "test_gen_training = datagen.flow_from_directory(valid_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=1,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)\n",
        "\n",
        "#--- Set Up the Generators Test\n",
        "test_path ='test_dir' \n",
        "# Here we change the path to point to the test_images folder.\n",
        "test_gen_testing = datagen.flow_from_directory(test_path,\n",
        "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                                        batch_size=1,\n",
        "                                        class_mode='categorical',\n",
        "                                        shuffle=False)\n",
        "\n",
        "#-- Create the Model Architecture\n",
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "\n",
        "dropout_conv = 0.3\n",
        "dropout_dense = 0.3\n",
        "\n",
        "# OBS. 'relu': função linear por partes que produzirá a entrada diretamente se for positiva, caso contrário, ela produzirá zero\n",
        "model = Sequential()\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3))) \n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu')) \n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size)) \n",
        "model.add(Dropout(dropout_conv)) \n",
        "\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
        "model.add(MaxPooling2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\")) \n",
        "model.add(Dropout(dropout_dense)) \n",
        "model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "#--- Train the Model\n",
        "model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Get the labels that are associated with each index\n",
        "print(val_gen.class_indices)\n",
        "\n",
        "filepath = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
        "                             save_best_only=True, mode='max')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, \n",
        "                                   verbose=1, mode='max', min_lr=0.00001)\n",
        "                  \n",
        "callbacks_list = [checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit_generator(train_gen, \n",
        "                    steps_per_epoch=train_steps, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=20, verbose=1,\n",
        "                    callbacks=callbacks_list)\n",
        "\n",
        "#--- Evaluate the model using the val set\n",
        "# get the metric names so we can use evaulate_generator\n",
        "model.metrics_names\n",
        "\n",
        "#--- Write and Read JSON model file\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        " \n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# Here the best epoch will be used.\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "val_loss, val_acc = \\\n",
        "model.evaluate_generator(test_gen_training, \n",
        "                        steps=len(df_val))\n",
        "\n",
        "print('val_loss:', val_loss)\n",
        "print('val_acc:', val_acc)\n",
        "\n",
        "#--- Make a prediction on the val set\n",
        "# We need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score\n",
        "# make a prediction\n",
        "predictions = model.predict_generator(test_gen_training, steps=len(df_val), verbose=1)\n",
        "print(predictions.shape)\n",
        "\n",
        "\n",
        "#--- A note on Keras class index values\n",
        "# Keras assigns it's own index value (here 0 and 1) to the classes. It infers the classes based on the folder structure.\n",
        "# Important: These index values may not match the index values we were given in the train_labels.csv file.\n",
        "# I've used 'a' and 'b' folder name pre-fixes to get keras to assign index values to match what was in the train_labels.csv file\n",
        "# I guessed that keras is assigning the index value based on folder name alphabetical order.\n",
        "\n",
        "# This is how to check what index keras has internally assigned to each class. \n",
        "print(test_gen_training.class_indices)\n",
        "\n",
        "# Put the predictions into a dataframe.\n",
        "# The columns need to be oredered to match the output of the previous cell\n",
        "df_preds = pd.DataFrame(predictions, columns=['seam_carving', 'seam_insertion', 'uncompressed'])\n",
        "print(df_preds.head())\n",
        "\n",
        "# Get the true labels\n",
        "y_true = test_gen_training.classes\n",
        "# print(y_true)\n",
        "# print(y_true.shape)\n",
        "\n",
        "# Get the predicted labels as probabilities\n",
        "# y_pred = df_preds['uncompressed']\n",
        "# print(y_pred)\n",
        "# print(y_pred.shape)\n",
        "\n",
        "#--- What is the AUC Score?\n",
        "# roc_auc_score(y_true, y_pred, multi_class='ovr') \n",
        "\n",
        "#--- Create a Confusion Matrix\n",
        "# Get the labels of the test images.\n",
        "test_labels_training = test_gen_training.classes\n",
        "print(test_labels_training.shape)\n",
        "print(test_labels_training)\n",
        "\n",
        "# Print the label associated with each class\n",
        "print(test_gen_training.class_indices)\n",
        "\n",
        "# argmax returns the index of the max value in a row\n",
        "# cm = confusion_matrix(test_labels_training, predictions.argmax(axis=1))\n",
        "cm = confusion_matrix(test_labels_training, predictions.argmax(axis=1))\n",
        "# Define the labels of the class indices. These need to match the \n",
        "# order shown above.\n",
        "cm_plot_labels = ['seam_carving', 'seam_insertion', 'uncompressed']\n",
        "plot_confusion_matrix(cm, cm_plot_labels)\n",
        "\n",
        "\n",
        "#--- Create a Classification Report\n",
        "# Generate a classification report\n",
        "# For this to work we need y_pred as categorical labels not as probabilities\n",
        "y_pred_categorical = predictions.argmax(axis=1)\n",
        "report = classification_report(y_true, y_pred_categorical, target_names=cm_plot_labels, labels=[0, 1, 2] )\n",
        "print(report)\n",
        "\n",
        "# Recall = Given a class, will the classifier be able to detect it?\n",
        "# Precision = Given a class prediction from a classifier, how likely is it to be correct?\n",
        "# F1 Score = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n",
        "# From the confusion matrix and classification report we see that our model is equally good at detecting both classes.\n",
        "\n",
        "# Are the number of predictions correct?\n",
        "# Should be 57458.\n",
        "print(len(predictions))\n",
        "# Put the predictions into a dataframe\n",
        "df_preds = pd.DataFrame(predictions, columns=['seam_carving', 'seam_insertion', 'uncompressed'])\n",
        "# print(df_preds.head())\n",
        "\n",
        "# This outputs the file names in the sequence in which \n",
        "# the generator processed the test images.\n",
        "test_filenames = test_gen_testing.filenames\n",
        "test_filenames = pd.DataFrame(test_filenames)\n",
        "print(test_filenames)\n",
        "print(test_filenames.shape)\n",
        "# add the filenames to the dataframe\n",
        "\n",
        "df_preds.insert(loc=3, column='file_names', value=test_filenames)\n",
        "# df_preds['file_names'] = test_filenames\n",
        "print(df_preds.head())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}